{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "In this notebook we explore techniques to clean and convert text features into numerical features that machine learning algorithms can work with. We will implement and explore the following.\n",
    "\n",
    "1. Common text pre-processing\n",
    "2. Lexicon based text processing\n",
    "3. Feature Extraction - Bag of words\n",
    "4. Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Common text pre-processing\n",
    "\n",
    "In this section, we will do some general purpose text cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a message to be cleaned. It may involve some things like: <br>, ?, :, ', '' adjacent spaces and tabs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a message to be cleaned. it may involve some things like: <br>, ?, :, ', '' adjacent spaces and tabs\n"
     ]
    }
   ],
   "source": [
    "# let's first lowercase our text completely.\n",
    "\n",
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a message to be cleaned. it may involve some things like: <br>, ?, :, ', '' adjacent spaces and tabs\n"
     ]
    }
   ],
   "source": [
    "# Now lets get rid of leading/trailing whitespaces\n",
    "# Note - In order to remove it just from from the left or right\n",
    "# side we can do lstrip and rstrip. But in this case we do\n",
    "\n",
    "text = text.strip()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a message to be cleaned it may involve some things like  br          adjacent spaces and tabs\n"
     ]
    }
   ],
   "source": [
    "import re, string\n",
    "\n",
    "text = re.compile('<.*?>').sub('', text)\n",
    "text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a message to be cleaned it may involve some things like br adjacent spaces and tabs\n"
     ]
    }
   ],
   "source": [
    "# Remove extra spaces and tabs\n",
    "\n",
    "import re\n",
    "\n",
    "text = re.sub('\\s+',' ', text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Lexicon based-tree processing\n",
    "\n",
    "We saw some general purpose text pre-processing methods. Lexicon methods are usually used to normalize sentences in our dataset and later we will use these normalized sentences for feature extraction.\n",
    "By normalizing here we mean putting words in the sentences into a similar format that will enhance similarities (if any) between sentences.\n",
    "\n",
    "Stop word removal : There can be some words in our sentences that occur very frequently and dont contribute too much to the overall meaning of the sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['message', 'be', 'cleaned', 'may', 'involve', 'some', 'things', 'like', 'br', 'adjacent', 'spaces', 'tabs']\n",
      "message be cleaned may involve some things like adjacent spaces tabs\n"
     ]
    }
   ],
   "source": [
    "stop_words = [\"a\", \"an\", \"the\", \"this\", \"that\", \"is\", \"it\", \"to\", \"and\", \"br\"]\n",
    "\n",
    "filtered_sentence = []\n",
    "words = text.split(\" \")\n",
    "print(words)\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "text = \" \".join(filtered_sentence)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Stemming</b> : Stemming is a rule based system to <b>convert words into their root form</b>.\n",
    "It removes suffixes from words. This helps us enhance similarities (if any) between sentences.\n",
    "\n",
    "Example:\n",
    "\n",
    "\"jumping\", \"jumped\" -> \"jump\"\n",
    "\"cars\" -> \"car\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messag be clean may involv some thing like adjac space tab\n"
     ]
    }
   ],
   "source": [
    "# we use the nltk library for stemming (Natural Language Toolkit)\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "# Initialize the stemmer\n",
    "ss = SnowballStemmer(\"english\")\n",
    "stemmed_sentence = []\n",
    "words = text.split(\" \")\n",
    "for w in words:\n",
    "    stemmed_sentence.append(ss.stem(w))\n",
    "text = \" \".join(stemmed_sentence)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Extraction - Bag of Words\n",
    "\n",
    "So the method is quite simple. FIrst we apply some common pre-processing methods and then we apply some lexicon based trasnformations. After those we will convert our text data into numerical data with the Bag of Words (BoW) representation.\n",
    "\n",
    "Bag of Words (BoW) : A modeling technique to convert text information into numerical representation.\n",
    "Machine Learning models expect numerical or categorical alues as input and won't work with raw text data.\n",
    "\n",
    "Steps :\n",
    "1. Create vocabulary of known words.\n",
    "2. Measure presence of the known words in sentences.\n",
    "\n",
    "We will use sklearn library's Bag of Words implementation.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countVectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "sentences = [\n",
    "    \"This is the first document\",\n",
    "    \"This is the second document\",\n",
    "    \"And the third one\",\n",
    "    \"Is this the first document\"\n",
    "]\n",
    "\n",
    "X = countVectorizer.fit_transform(sentences)\n",
    "\n",
    "print(countVectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each number next to a word represents its index in the vocabulary (from 0 to 8 in this case)\n",
    "\n",
    "Note : sklearn automatically removes punctuation, but doesnt do the other extra pre-processing methods we discussed.\n",
    "Lexicon-based methods are also not automatically applied, we need to call those methods before feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 1 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what happens when we encounter a new word during prediction?\n",
    "\n",
    "New words will be skipped.\n",
    "This usually happens when we are making predictions, for our text and validation data/text, we need to use the .transform() function this time.\n",
    "This stimulates a real-time prediction case when we cannot re-train the model quickly whenever we receive new words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0 0 1]\n",
      " [0 0 0 1 1 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"this document has some new words\",\n",
    "    \"this one is new too\"\n",
    "]\n",
    "\n",
    "count_vectors = countVectorizer.transform(test_sentences)\n",
    "print(count_vectors.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Putting it all together\n",
    "\n",
    "Let's have a full example here. We will apply everything discussed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare cleaning functions\n",
    "\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stop_words = [\"a\", \"an\", \"the\", \"this\", \"that\", \"is\", \"it\", \"to\",\"and\"]\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def preProcessText(text):\n",
    "    \n",
    "    # lowercase and strip leading/training white space\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    # remove HTML tags\n",
    "    text = re.compile('<.*?>').sub('', text)\n",
    "    \n",
    "    # remove punctuations\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "    \n",
    "    # remove extra white spaces\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def lexiconProcess(text, stop_words, stemmer):\n",
    "    filtered_sentence = []\n",
    "    words = text.split(\" \")\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(stemmer.stem(w))\n",
    "    text = \" \".join(filtered_sentence)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def cleanSentence(text, stop_words, stemmer):\n",
    "    return lexiconProcess(preProcessText(text), stop_words, stemmer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "textVectorizer = CountVectorizer(binary=True) # can also limit\n",
    "#vocabulary size here, with say, max_features=50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "['i like materi color overal how look ', 'work okay fist two time i use but third time burn his face', 'i am not sure about product', 'i never thought i would pay so much for hair dryer']\n",
      "Vocabulary:\n",
      " {'like': 12, 'materi': 14, 'color': 4, 'overal': 19, 'how': 11, 'look': 13, 'work': 29, 'okay': 18, 'fist': 7, 'two': 27, 'time': 26, 'use': 28, 'but': 3, 'third': 24, 'burn': 2, 'his': 10, 'face': 6, 'am': 1, 'not': 17, 'sure': 23, 'about': 0, 'product': 21, 'never': 16, 'thought': 25, 'would': 30, 'pay': 20, 'so': 22, 'much': 15, 'for': 8, 'hair': 9, 'dryer': 5}\n",
      "Bag of words Binary features:\n",
      " [[0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1]]\n",
      "(4, 31)\n"
     ]
    }
   ],
   "source": [
    "# clean and vectorize a text feature with four samples\n",
    "\n",
    "text_feature = [\n",
    "    \"I liked the material, color and overall how it looks.<br /><br />\",\n",
    "    \"Worked okay fist two times I used it, but third time burned his face\",\n",
    "    \"I am not sure about this product\",\n",
    "    \"I never thought I would pay so much for a hair dryer\"\n",
    "\n",
    "]\n",
    "print(len(text_feature))\n",
    "\n",
    "# clean up the text\n",
    "\n",
    "text_feature_cleaned = [cleanSentence(\n",
    "    item, stop_words, stemmer) for item in text_feature]\n",
    "print(text_feature_cleaned)\n",
    "\n",
    "# Vectorize the cleaned text\n",
    "\n",
    "text_feature_vectorized = textVectorizer.fit_transform(text_feature_cleaned)\n",
    "\n",
    "print('Vocabulary:\\n', textVectorizer.vocabulary_)\n",
    "print('Bag of words Binary features:\\n', text_feature_vectorized.toarray())\n",
    "print(text_feature_vectorized.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
